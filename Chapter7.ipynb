{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter7.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "C7G0EXaieL6j",
        "feYQmZGSinM6",
        "bnceLttGL0Zh"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wllgrnt/keras-examples/blob/master/Chapter7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiPkrBggVRPX",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 7\n",
        "\n",
        "## Advanced deep-learning best practices\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4mUUJQNVhym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d9aac15f-9b7c-45f3-c420-d8ecc56bf996"
      },
      "source": [
        "import keras\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xzmXQN6ViSe",
        "colab_type": "text"
      },
      "source": [
        "## The Keras functional API\n",
        "\n",
        "If we want to go beyond to `Sequential` model (e.g. with independent inputs, multiple outputs, internal branching), we have to use the functional API.\n",
        "\n",
        "With the functional API, we treat layers as functions that take tensors and return tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HglyzZOjdNfD",
        "colab_type": "code",
        "outputId": "58da6868-7462-4861-8861-b47f840a9df7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# Example of the functional API - layer takes an input tensor, returns an output tensor.\n",
        "input_tensor = keras.Input(shape=(32,))\n",
        "dense = keras.layers.Dense(32, activation=\"relu\")\n",
        "output_tensor = dense(input_tensor)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0621 14:35:03.740296 140189048977280 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsfiquoHdkZe",
        "colab_type": "code",
        "outputId": "6a37234f-e5b7-4c5a-b60a-6e3af9c7c81c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "# A Sequential model and its functional API equivalent\n",
        "\n",
        "seq_model = keras.models.Sequential()\n",
        "seq_model.add(keras.layers.Dense(32, activation=\"relu\", input_shape=(64,)))\n",
        "seq_model.add(keras.layers.Dense(32, activation=\"relu\"))\n",
        "seq_model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "input_tensor = keras.Input(shape=(64,))\n",
        "x = keras.layers.Dense(32, activation=\"relu\")(input_tensor)\n",
        "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
        "output_tensor = keras.layers.Dense(10, activation=\"softmax\")(x)\n",
        "\n",
        "# Keras retrieves every layer involved in going from the input to the output,\n",
        "# and brings them together into a graph-like structure.\n",
        "model = keras.models.Model(input_tensor, output_tensor)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Once the Model is created, we compile, fit, evalute as before\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 3,466\n",
            "Trainable params: 3,466\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7G0EXaieL6j",
        "colab_type": "text"
      },
      "source": [
        "### Multi-input models\n",
        "\n",
        "Here we create an example question-answering model, in which the question and the reference material are both fed in to separate LSTMs and then combined to give an answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GROPHv9uenCP",
        "colab_type": "code",
        "outputId": "3b0a0369-32e1-4f0c-c34a-8021040508eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "text_vocabulary_size = 10000\n",
        "question_vocabulary_size = 10000\n",
        "answer_vocabulary_size = 500\n",
        "\n",
        "text_input = keras.Input(shape=(None,), dtype=\"int32\", name=\"text\")\n",
        "\n",
        "# Embed the inputs into a sequence of length-64 vectors\n",
        "embedded_text = keras.layers.Embedding(64, text_vocabulary_size)(text_input)\n",
        "\n",
        "# Encode into a single vector via an LSTM\n",
        "encoded_text = keras.layers.LSTM(32)(embedded_text)\n",
        "\n",
        "question_input = keras.Input(shape=(None,), dtype=\"int32\", name=\"question\")\n",
        "\n",
        "embedded_question = keras.layers.Embedding(32, question_vocabulary_size)(question_input)\n",
        "encoded_question = keras.layers.LSTM(16)(embedded_question)\n",
        "\n",
        "concatenated = keras.layers.concatenate([encoded_text, encoded_question], axis=-1)\n",
        "\n",
        "answer = keras.layers.Dense(answer_vocabulary_size, activation=\"softmax\")(concatenated)\n",
        "\n",
        "model = keras.models.Model([text_input, question_input], answer)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "text (InputLayer)               (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "question (InputLayer)           (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, None, 10000)  640000      text[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, None, 10000)  320000      question[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   (None, 32)           1284224     embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   (None, 16)           641088      embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 48)           0           lstm_3[0][0]                     \n",
            "                                                                 lstm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_22 (Dense)                (None, 500)          24500       concatenate_2[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 2,909,812\n",
            "Trainable params: 2,909,812\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYWeGowqhDTX",
        "colab_type": "text"
      },
      "source": [
        "We train this model either by feeding a list of NumPy arrays as inputs, or a dictionary mapping input names to Numpy arrays (assuming you named your inputs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA8H3QtkhQcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_samples = 1000\n",
        "max_length = 100\n",
        "\n",
        "# Generate dummy Numpy data\n",
        "text = np.random.randint(1, text_vocabulary_size, size=(num_samples, max_length))\n",
        "\n",
        "question = np.random.randint(1, question_vocabulary_size, size=(num_samples, max_length))\n",
        "\n",
        "# Answers are one-hot encoded, not integers\n",
        "answers = np.random.randint(0,1, size=(num_samples, answer_vocabulary_size))\n",
        "\n",
        "# Option one\n",
        "#model.fit([text, question], answers, epochs=10, batch_size=128)\n",
        "\n",
        "# Option two\n",
        "model.fit({\"text\": text, \"question\": question}, answers, epochs=10,x batch_size=128)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feYQmZGSinM6",
        "colab_type": "text"
      },
      "source": [
        "### Multi-output models\n",
        "\n",
        "e.g. A network that attempts to simultaneously predict different properties of the data, such as taking a series of social media posts from a single anonymous individual  and trying to predict their attributes (spooky)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ghySGlR2z2e",
        "colab_type": "code",
        "outputId": "d636af7a-6114-453b-ab56-b8ad61474451",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "vocabulary_size = 50000\n",
        "num_income_groups = 10\n",
        "\n",
        "posts_input = keras.Input(shape=(None,), dtype=\"int32\", name=\"posts\")\n",
        "embedded_posts = keras.layers.Embedding(256, vocabulary_size)(posts_input)\n",
        "x = keras.layers.Conv1D(128, 5, activation=\"relu\")(embedded_posts)\n",
        "x = keras.layers.MaxPooling1D(5)(x)\n",
        "x = keras.layers.Conv1D(256, 5, activation=\"relu\")(x)\n",
        "x = keras.layers.Conv1D(256, 5, activation=\"relu\")(x)\n",
        "x = keras.layers.MaxPooling1D(5)(x)\n",
        "x = keras.layers.Conv1D(256, 5, activation=\"relu\")(x)\n",
        "x = keras.layers.Conv1D(256, 5, activation=\"relu\")(x)\n",
        "x = keras.layers.GlobalMaxPooling1D()(x)\n",
        "x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "\n",
        "age_prediction = keras.layers.Dense(1, name=\"age\")(x)\n",
        "income_prediction = keras.layers.Dense(num_income_groups, activation=\"softmax\", name=\"income\")(x)\n",
        "gender_prediction = keras.layers.Dense(1, activation=\"sigmoid\", name=\"gender\")(x)\n",
        "\n",
        "model = keras.models.Model(posts_input, [age_prediction, income_prediction, gender_prediction])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0625 10:26:41.459146 140193643763584 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0625 10:26:41.537805 140193643763584 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0625 10:26:41.539778 140193643763584 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0625 10:26:41.612673 140193643763584 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5F-iWhsMRRw",
        "colab_type": "text"
      },
      "source": [
        "To train this model, we need different loss functions for different heads of the network. But gradient descent requires a scalar, so we combine all the losses by summing them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrKekENZ4MUO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "5030fd66-a174-43ec-bf38-4c8ffa6a496f"
      },
      "source": [
        "# Option 1\n",
        "# model.compile(optimizer=\"rmsprop\",\n",
        "#              loss=[\"mse\", \"categorical_crossentropy\", \"binary_crossentropy\"])\n",
        "\n",
        "# Option 2\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss={\"age\": \"mse\",\n",
        "                    \"income\": \"categorical_crossentropy\",\n",
        "                    \"gender\": \"binary_crossentropy\"}\n",
        "             )\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0625 10:28:23.992760 140193643763584 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0625 10:28:24.041905 140193643763584 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0625 10:28:24.072448 140193643763584 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nev_I9HOMFVi",
        "colab_type": "text"
      },
      "source": [
        "We can weight the sum so that each loss has a different importance - This is useful when the losses' values have different scales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqxp6z6aLfAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss={\"age\": \"mse\",\n",
        "                    \"income\": \"categorical_crossentropy\",\n",
        "                    \"gender\": \"binary_crossentropy\"},\n",
        "              loss_weights={\"age\": 0.25,\n",
        "                    \"income\": 1.0,\n",
        "                    \"gender\": 10.0}\n",
        "             )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnceLttGL0Zh",
        "colab_type": "text"
      },
      "source": [
        "We fit as before by passing an list of arrays of a dict of arrays.\n",
        "\n",
        "```\n",
        "model.fit(posts, {\"age\": age_targets,\n",
        "                  \"income\": income_targets,\n",
        "                  \"gender\": gender_targets},\n",
        "          epochs=10, batch_size=64)\n",
        "\n",
        "```\n",
        "\n",
        "### Directed acyclic graphs of layers\n",
        "\n",
        "In addition to multiple-input and multiple-output networks, we can have any arbitrary directed acyclic graph of layers as our internal network topology.\n",
        "\n",
        "Notable examples of neural-network components that are implemented as graphs include the Inception module and residual connections.\n",
        "\n",
        "\n",
        "#### Inception modules\n",
        "\n",
        "This is a stack of modules, each of which looks like a small independent network, split into branches. E.g. three branches, starting with a 1x1 convolution, followed by a 3x3 convolution, then finally concatenating the resulting features. This  helps the network learn spatial features and channel-wise features separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVBdP79eMrtr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example implementation of an Inception module (assyming 4d input tensor x)\n",
        "\n",
        "x = keras.Input(shape=(None, None, 3), dtype=\"float32\", name=\"input\")\n",
        "\n",
        "\n",
        "branch_a = keras.layers.Conv2D(128, 1, activation=\"relu\", strides=2)(x)\n",
        "\n",
        "branch_b = keras.layers.Conv2D(128, 1, activation=\"relu\")(x)\n",
        "branch_b = keras.layers.Conv2D(128, 3, activation=\"relu\", strides=2)(branch_b)\n",
        "\n",
        "\n",
        "branch_c = keras.layers.AveragePooling2D(3, strides=2)(x)\n",
        "branch_c = keras.layers.Conv2D(128, 3, activation=\"relu\")(branch_c)\n",
        "\n",
        "branch_d = keras.layers.Conv2D(128, 1, activation=\"relu\")(x)\n",
        "branch_d = keras.layers.Conv2D(128, 3, activation=\"relu\")(branch_d)\n",
        "branch_d = keras.layers.Conv2D(128, 3, activation=\"relu\", strides=2)(branch_d)\n",
        "\n",
        "output = keras.layers.concatenate([branch_a, branch_b, branch_c, branch_d], axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhrqe-hQQdel",
        "colab_type": "text"
      },
      "source": [
        "#### Residual connections\n",
        "\n",
        "This network component tackles vanishing gradients and representational bottlenecks , by making the output of an earlier layer available as an input to a later layer - the earlier output is summed with the later activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUEcnLDcSikr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example implementation of a residual connection when the feature-map sizes are the same\n",
        "# Uses identity residual connections\n",
        "\n",
        "x = keras.Input(shape=(None, None, 128), dtype=\"float32\", name=\"input\")\n",
        "\n",
        "\n",
        "y = keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "y = keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(y)\n",
        "y = keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(y)\n",
        "\n",
        "# Add the original x back to the output features\n",
        "y = keras.layers.add([y,x])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nHbx1HXS3iO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example implementation when the feature map sizes differ, using a linear residual connection\n",
        "\n",
        "x = keras.Input(shape=(None, None, 3), dtype=\"float32\", name=\"input\")\n",
        "\n",
        "\n",
        "y = keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "y = keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(y)\n",
        "y = keras.layers.MaxPooling2D(2,strides=2)(y)\n",
        "\n",
        "\n",
        "# Use a 1x1 convolution to linearly downsample the original x tensor to the same shape as y\n",
        "residual = keras.layers.Conv2D(128, 1, strides=2, padding=\"same\")(x)\n",
        "\n",
        "# Add the original x back to the output features\n",
        "y = keras.layers.add([y,residual])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMf3Gb95T5AZ",
        "colab_type": "text"
      },
      "source": [
        "### Layer weight sharing\n",
        "\n",
        "If we call the same layer twice, we reuse the same weights with every call - this lets us share branches. E.g. with a Siamese LSTM (?) aka a shared LSTM, where we process two inputs with a single LSTM layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QfVag4FUVhn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "1e766c78-b1b3-47bf-9655-ff7608b71cd5"
      },
      "source": [
        "lstm = keras.layers.LSTM(32)\n",
        "\n",
        "# Variable-length sequences of vectors of size 128\n",
        "left_input = keras.Input(shape=(None, 128))\n",
        "left_output = lstm(left_input)\n",
        "\n",
        "right_input = keras.Input(shape=(None, 128))\n",
        "right_output = lstm(right_input)\n",
        "\n",
        "\n",
        "\n",
        "merged = keras.layers.concatenate([left_output, right_output], axis=-1)\n",
        "\n",
        "predictions = keras.layers.Dense(1, activation=\"sigmoid\")(merged)\n",
        "\n",
        "model = keras.models.Model([left_input, right_input], predictions)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# model.fit([left_data, right_data], targets)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, None, 128)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            (None, None, 128)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 32)           20608       input_3[0][0]                    \n",
            "                                                                 input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 64)           0           lstm_2[0][0]                     \n",
            "                                                                 lstm_2[1][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            65          concatenate_4[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 20,673\n",
            "Trainable params: 20,673\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4v7RW4YWTft",
        "colab_type": "text"
      },
      "source": [
        "### Models as layers\n",
        "\n",
        "We can think of models as a \"bigger layer\", e..g `y = model(x)`, and again the weights are shared. E.g. for a vision model with a dual camera: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1Z1I7FaXr4m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "6d95f7bd-e253-4662-a092-c87515cfe7b8"
      },
      "source": [
        "xception_base = keras.applications.Xception(weights=None, include_top=False)\n",
        "\n",
        "# 250x250 rgb images\n",
        "left_input = keras.Input(shape=(250,250,3))\n",
        "right_input = keras.Input(shape=(250,250,3))\n",
        "\n",
        "left_features = xception_base(left_input)\n",
        "right_features = xception_base(right_input)\n",
        "\n",
        "merged_features = keras.layers.concatenate([left_features, right_features], axis=-1)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0625 11:25:35.062937 140193643763584 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0625 11:25:36.231467 140193643763584 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAu6PVEOYkrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}